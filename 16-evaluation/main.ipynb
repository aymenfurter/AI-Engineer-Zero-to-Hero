{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf7bd04",
   "metadata": {},
   "source": [
    "# Lab 16: Evaluate Your Generative AI Application\n",
    "\n",
    "> ‚ö†Ô∏è **In Development**: This notebook is still being developed and is not ready for use yet. Content and APIs may change significantly.\n",
    "\n",
    "Use the **Azure AI Evaluation SDK** to assess the quality and safety of your AI applications!\n",
    "\n",
    "\n",
    "## What is Azure AI Evaluation?\n",
    "\n",
    "| Challenge | Solution |\n",
    "|-----------|----------|\n",
    "| How good are my agent's responses? | **Quality evaluators** (coherence, fluency, relevance) |\n",
    "| Are responses grounded in facts? | **Groundedness evaluators** detect hallucinations |\n",
    "| Is my agent safe to deploy? | **Safety evaluators** check for harmful content |\n",
    "| How do I measure at scale? | **Batch evaluation** with `evaluate()` API |\n",
    "\n",
    "## Features Demonstrated\n",
    "\n",
    "- **Built-in Evaluators** - Quality metrics (coherence, fluency, relevance, groundedness)\n",
    "- **Custom Evaluators** - Create your own evaluation logic\n",
    "- **Batch Evaluation** - Run evaluators on entire test datasets\n",
    "- **Agent Evaluation** - Test the Space Expert agent from Lab 6\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed **Lab 1a** (Landing Zone with APIM)\n",
    "- Completed **Lab 6** (Foundry IQ - Space Expert Agent)\n",
    "- `.env` file with APIM_URL and APIM_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6625ec8",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa339b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-ai-evaluation azure-ai-projects azure-identity pandas requests -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bed710b",
   "metadata": {},
   "source": [
    "## Step 2: Configure Variables\n",
    "\n",
    "Load configuration from the parent `.env` file and Lab 6's deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b469b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Load .env from parent directory\n",
    "env_path = Path(\"../.env\")\n",
    "if env_path.exists():\n",
    "    for line in env_path.read_text().splitlines():\n",
    "        if '=' in line and not line.startswith('#'):\n",
    "            key, value = line.split('=', 1)\n",
    "            os.environ[key.strip()] = value.strip()\n",
    "\n",
    "# Landing Zone settings (from Lab 1a)\n",
    "APIM_URL = os.environ.get(\"APIM_URL\", \"\")\n",
    "APIM_KEY = os.environ.get(\"APIM_KEY\", \"\")\n",
    "MODEL_NAME = os.environ.get(\"MODEL_NAME\", \"gpt-4.1-mini\")\n",
    "\n",
    "# Lab 6 resource group\n",
    "RG = \"foundryiq-lab\"\n",
    "\n",
    "# Get subscription ID\n",
    "SUBSCRIPTION_ID = subprocess.run(\n",
    "    'az account show --query id -o tsv',\n",
    "    shell=True, capture_output=True, text=True\n",
    ").stdout.strip()\n",
    "\n",
    "# Verify configuration\n",
    "if not APIM_URL or not APIM_KEY:\n",
    "    print(\"‚ùå Missing APIM_URL or APIM_KEY in .env file!\")\n",
    "    print(\"   Please complete Lab 1a first\")\n",
    "else:\n",
    "    display(Markdown(f'''\n",
    "### ‚úÖ Configuration Loaded\n",
    "\n",
    "| Setting | Value |\n",
    "|---------|-------|\n",
    "| APIM Gateway | `{APIM_URL[:50]}...` |\n",
    "| Evaluator Model | `{MODEL_NAME}` |\n",
    "| Resource Group | `{RG}` |\n",
    "'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d0bf2",
   "metadata": {},
   "source": [
    "## Step 3: Load Lab 6 Deployment Info\n",
    "\n",
    "Get the Foundry IQ project details from Lab 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fdff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get deployment outputs from Lab 6\n",
    "try:\n",
    "    outputs = json.loads(subprocess.run(\n",
    "        f'az deployment group show -g \"{RG}\" -n spoke --query properties.outputs -o json',\n",
    "        shell=True, capture_output=True, text=True\n",
    "    ).stdout)\n",
    "\n",
    "    PROJECT_ENDPOINT = outputs['projectEndpoint']['value']\n",
    "    APIM_CONNECTION = outputs['apimConnectionName']['value']\n",
    "    SEARCH_ENDPOINT = outputs['searchEndpoint']['value']\n",
    "    GATEWAY_MODEL = f\"{APIM_CONNECTION}/{outputs['gatewayModelName']['value']}\"\n",
    "    KNOWLEDGE_BASE = \"space-facts-kb\"\n",
    "\n",
    "    display(Markdown(f'''\n",
    "### ‚úÖ Lab 6 Resources Found\n",
    "\n",
    "| Resource | Value |\n",
    "|----------|-------|\n",
    "| Project Endpoint | `{PROJECT_ENDPOINT[:50]}...` |\n",
    "| Gateway Model | `{GATEWAY_MODEL}` |\n",
    "| Knowledge Base | `{KNOWLEDGE_BASE}` |\n",
    "'''))\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Could not load Lab 6 deployment: {e}\")\n",
    "    print(\"   Please complete Lab 6 first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf50c99",
   "metadata": {},
   "source": [
    "## Step 4: Set Up Model Configuration for Evaluators\n",
    "\n",
    "AI-assisted evaluators need a model to act as a \"judge\". We'll use the APIM gateway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00159384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model configuration ready!\n",
      "   Using: gpt-4.1-mini via APIM gateway\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "\n",
    "# Configure the evaluator model using APIM gateway\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=APIM_URL.replace('/openai', ''),\n",
    "    api_key=APIM_KEY,\n",
    "    azure_deployment=MODEL_NAME,\n",
    "    api_version=\"2024-10-21\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model configuration ready!\")\n",
    "print(f\"   Using: {MODEL_NAME} via APIM gateway\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180b1c51",
   "metadata": {},
   "source": [
    "## Step 5: Connect to Space Expert Agent\n",
    "\n",
    "Connect to the agent we created in Lab 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c80f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "project_client = AIProjectClient(endpoint=PROJECT_ENDPOINT, credential=credential)\n",
    "openai_client = project_client.get_openai_client()\n",
    "\n",
    "# Get the Space Expert agent\n",
    "AGENT_NAME = \"SpaceExpert\"\n",
    "agent = project_client.agents.get(agent_name=AGENT_NAME)\n",
    "agent_version = agent.versions.latest.version\n",
    "\n",
    "print(f\"‚úÖ Connected to agent: {agent.name} v{agent_version}\")\n",
    "\n",
    "def ask_space_expert(question: str) -> str:\n",
    "    \"\"\"Ask the space expert a question.\"\"\"\n",
    "    response = openai_client.responses.create(\n",
    "        input=question,\n",
    "        extra_body={\n",
    "            \"agent\": {\n",
    "                \"name\": agent.name, \n",
    "                \"version\": agent_version, \n",
    "                \"type\": \"agent_reference\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return response.output_text\n",
    "\n",
    "# Test the connection\n",
    "test_response = ask_space_expert(\"What is the Apollo 14 mission?\")\n",
    "print(f\"\\nüì° Test response: {test_response[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca57cc1",
   "metadata": {},
   "source": [
    "## Step 6: Create Test Dataset\n",
    "\n",
    "We'll create a test dataset based on our space facts, with queries, expected answers (ground truth), and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b191dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### üìã Test Dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the largest volcano in the solar system?</td>\n",
       "      <td>Olympus Mons on Mars is the largest volcano in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How long has Jupiter's Great Red Spot been act...</td>\n",
       "      <td>Jupiter's Great Red Spot has been raging for o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why is a day on Venus longer than its year?</td>\n",
       "      <td>Venus takes 243 Earth days to rotate once but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How much of the solar system's mass does the S...</td>\n",
       "      <td>The Sun contains 99.86% of all mass in our sol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How fast does the International Space Station ...</td>\n",
       "      <td>The ISS travels at about 17,500 mph and comple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What happens to astronauts' height in space?</td>\n",
       "      <td>Astronauts grow up to 2 inches taller in space...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0   What is the largest volcano in the solar system?   \n",
       "1  How long has Jupiter's Great Red Spot been act...   \n",
       "2        Why is a day on Venus longer than its year?   \n",
       "3  How much of the solar system's mass does the S...   \n",
       "4  How fast does the International Space Station ...   \n",
       "5       What happens to astronauts' height in space?   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  Olympus Mons on Mars is the largest volcano in...  \n",
       "1  Jupiter's Great Red Spot has been raging for o...  \n",
       "2  Venus takes 243 Earth days to rotate once but ...  \n",
       "3  The Sun contains 99.86% of all mass in our sol...  \n",
       "4  The ISS travels at about 17,500 mph and comple...  \n",
       "5  Astronauts grow up to 2 inches taller in space...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Test dataset with queries, ground truth, and context\n",
    "test_data = [\n",
    "    {\n",
    "        \"query\": \"What is the largest volcano in the solar system?\",\n",
    "        \"ground_truth\": \"Olympus Mons on Mars is the largest volcano in the solar system, about 13.6 miles high.\",\n",
    "        \"context\": \"Mars has the largest volcano in the solar system called Olympus Mons which is about 13.6 miles high.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How long has Jupiter's Great Red Spot been active?\",\n",
    "        \"ground_truth\": \"Jupiter's Great Red Spot has been raging for over 400 years.\",\n",
    "        \"context\": \"Jupiter's Great Red Spot is a storm that has been raging for over 400 years and is so big that Earth could fit inside it.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Why is a day on Venus longer than its year?\",\n",
    "        \"ground_truth\": \"Venus takes 243 Earth days to rotate once but only 225 Earth days to orbit the Sun, making its day longer than its year.\",\n",
    "        \"context\": \"A day on Venus is longer than its year! Venus takes 243 Earth days to rotate once but only 225 Earth days to orbit the Sun.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How much of the solar system's mass does the Sun contain?\",\n",
    "        \"ground_truth\": \"The Sun contains 99.86% of all mass in our solar system.\",\n",
    "        \"context\": \"The Sun contains 99.86% of all mass in our solar system.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How fast does the International Space Station travel?\",\n",
    "        \"ground_truth\": \"The ISS travels at about 17,500 mph and completes one orbit every 90 minutes.\",\n",
    "        \"context\": \"The International Space Station travels at about 17500 mph completing one orbit around Earth every 90 minutes.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What happens to astronauts' height in space?\",\n",
    "        \"ground_truth\": \"Astronauts grow up to 2 inches taller in space because there is no gravity compressing their spines.\",\n",
    "        \"context\": \"Astronauts grow up to 2 inches taller in space because there is no gravity compressing their spines.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "df_test = pd.DataFrame(test_data)\n",
    "display(Markdown(\"### üìã Test Dataset\"))\n",
    "display(df_test[['query', 'ground_truth']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff6d09",
   "metadata": {},
   "source": [
    "## Step 7: Generate Agent Responses\n",
    "\n",
    "Run the test queries through our Space Expert agent to get responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8907c14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Generating agent responses...\n",
      "  Query 1/6: What is the largest volcano in the solar system?...\n",
      "  Query 2/6: How long has Jupiter's Great Red Spot been active?...\n",
      "  Query 3/6: Why is a day on Venus longer than its year?...\n",
      "  Query 4/6: How much of the solar system's mass does the Sun c...\n",
      "  Query 5/6: How fast does the International Space Station trav...\n",
      "  Query 6/6: What happens to astronauts' height in space?...\n",
      "\n",
      "‚úÖ All responses generated!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Sample Response"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Query:** What is the largest volcano in the solar system?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Response:** The largest volcano in the solar system is Olympus Mons on Mars. It is about 13.6 miles (approximately 22 kilometers) high, making it the tallest volcano known in our solar system. \n",
       "\n",
       "Source: fact-010"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"ü§ñ Generating agent responses...\")\n",
    "responses = []\n",
    "\n",
    "for i, row in df_test.iterrows():\n",
    "    print(f\"  Query {i+1}/{len(df_test)}: {row['query'][:50]}...\")\n",
    "    response = ask_space_expert(row['query'])\n",
    "    responses.append(response)\n",
    "\n",
    "df_test['response'] = responses\n",
    "\n",
    "print(\"\\n‚úÖ All responses generated!\")\n",
    "display(Markdown(\"### Sample Response\"))\n",
    "display(Markdown(f\"**Query:** {df_test.iloc[0]['query']}\"))\n",
    "display(Markdown(f\"**Response:** {df_test.iloc[0]['response']}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91eff4e",
   "metadata": {},
   "source": [
    "## Step 8: Save Test Data as JSONL\n",
    "\n",
    "The `evaluate()` API expects data in JSONL format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1643c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test data saved to test_data.jsonl\n",
      "\n",
      "üìÑ Sample JSONL entry:\n",
      "{\n",
      "  \"query\": \"What is the largest volcano in the solar system?\",\n",
      "  \"ground_truth\": \"Olympus Mons on Mars is the largest volcano in the solar system, about 13.6 miles high.\",\n",
      "  \"context\": \"Mars has the largest volcano in the solar system called Olympus Mons which is about 13.6 miles high.\",\n",
      "  \"response\": \"The largest volcano in the solar system is Olympus Mons on Mars. It is about 13.6 miles (approximately 22 kilometers) high, making it the tallest volcano known in our solar system. \\n\\nSource: f...\n"
     ]
    }
   ],
   "source": [
    "# Save test data to JSONL\n",
    "df_test.to_json('test_data.jsonl', orient='records', lines=True)\n",
    "print(\"‚úÖ Test data saved to test_data.jsonl\")\n",
    "\n",
    "# Show sample line\n",
    "with open('test_data.jsonl', 'r') as f:\n",
    "    first_line = json.loads(f.readline())\n",
    "    print(\"\\nüìÑ Sample JSONL entry:\")\n",
    "    print(json.dumps(first_line, indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c2653",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Single-Row Evaluation (Spot Check)\n",
    "\n",
    "Before running batch evaluation, let's test individual evaluators on a single row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294be857",
   "metadata": {},
   "source": [
    "## Step 9: Quality Evaluators - Coherence & Fluency\n",
    "\n",
    "These evaluators assess how well-written and readable the responses are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6121f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### üéØ Single-Row Quality Evaluation\n",
       "\n",
       "**Query:** What is the largest volcano in the solar system?\n",
       "\n",
       "**Response:** The largest volcano in the solar system is Olympus Mons on Mars. It is about 13.6 miles (approximately 22 kilometers) high, making it the tallest volcano known in our solar system. \n",
       "\n",
       "Source: fact-010...\n",
       "\n",
       "| Metric | Score | Reason |\n",
       "|--------|-------|--------|\n",
       "| **Coherence** | 4.0 | The response is coherent because it logically and clearly answers the question with relevant details... |\n",
       "| **Fluency** | 3.0 | The response is clear, coherent, and grammatically correct with adequate vocabulary and sentence str... |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azure.ai.evaluation import CoherenceEvaluator, FluencyEvaluator\n",
    "\n",
    "# Initialize evaluators\n",
    "coherence_eval = CoherenceEvaluator(model_config)\n",
    "fluency_eval = FluencyEvaluator(model_config)\n",
    "\n",
    "# Test on first sample\n",
    "sample = df_test.iloc[0]\n",
    "\n",
    "coherence_result = coherence_eval(\n",
    "    query=sample['query'],\n",
    "    response=sample['response']\n",
    ")\n",
    "\n",
    "fluency_result = fluency_eval(\n",
    "    query=sample['query'],\n",
    "    response=sample['response']\n",
    ")\n",
    "\n",
    "display(Markdown(f'''\n",
    "### üéØ Single-Row Quality Evaluation\n",
    "\n",
    "**Query:** {sample['query']}\n",
    "\n",
    "**Response:** {sample['response'][:200]}...\n",
    "\n",
    "| Metric | Score | Reason |\n",
    "|--------|-------|--------|\n",
    "| **Coherence** | {coherence_result.get('coherence', 'N/A')} | {coherence_result.get('coherence_reason', 'N/A')[:100]}... |\n",
    "| **Fluency** | {fluency_result.get('fluency', 'N/A')} | {fluency_result.get('fluency_reason', 'N/A')[:100]}... |\n",
    "'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61ed0b8",
   "metadata": {},
   "source": [
    "## Step 10: Relevance Evaluator\n",
    "\n",
    "Does the response actually answer the question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f14775c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### üéØ Relevance Evaluation\n",
       "\n",
       "| Metric | Score |\n",
       "|--------|-------|\n",
       "| **Relevance** | 4.0 |\n",
       "\n",
       "**Reason:** The response correctly identifies Olympus Mons as the largest volcano in the solar system and provides its height, fully answering the question with accurate and complete information.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "\n",
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "\n",
    "relevance_result = relevance_eval(\n",
    "    query=sample['query'],\n",
    "    response=sample['response']\n",
    ")\n",
    "\n",
    "display(Markdown(f'''\n",
    "### üéØ Relevance Evaluation\n",
    "\n",
    "| Metric | Score |\n",
    "|--------|-------|\n",
    "| **Relevance** | {relevance_result.get('relevance', 'N/A')} |\n",
    "\n",
    "**Reason:** {relevance_result.get('relevance_reason', 'N/A')}\n",
    "'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b15cd6",
   "metadata": {},
   "source": [
    "## Step 11: Groundedness Evaluator\n",
    "\n",
    "Is the response grounded in the provided context? This detects hallucinations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cf76f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### üéØ Groundedness Evaluation\n",
       "\n",
       "**Context:** Mars has the largest volcano in the solar system called Olympus Mons which is about 13.6 miles high.\n",
       "\n",
       "| Metric | Score | Pass/Fail |\n",
       "|--------|-------|-----------|  \n",
       "| **Groundedness** | 5.0 | N/A |\n",
       "\n",
       "**Reason:** The response fully and accurately answers the question using all relevant details from the context, making it a complete and correct answer.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azure.ai.evaluation import GroundednessEvaluator\n",
    "\n",
    "groundedness_eval = GroundednessEvaluator(model_config)\n",
    "\n",
    "groundedness_result = groundedness_eval(\n",
    "    query=sample['query'],\n",
    "    context=sample['context'],\n",
    "    response=sample['response']\n",
    ")\n",
    "\n",
    "display(Markdown(f'''\n",
    "### üéØ Groundedness Evaluation\n",
    "\n",
    "**Context:** {sample['context']}\n",
    "\n",
    "| Metric | Score | Pass/Fail |\n",
    "|--------|-------|-----------|  \n",
    "| **Groundedness** | {groundedness_result.get('groundedness', 'N/A')} | {groundedness_result.get('groundedness_result', 'N/A')} |\n",
    "\n",
    "**Reason:** {groundedness_result.get('groundedness_reason', 'N/A')}\n",
    "'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57728d8a",
   "metadata": {},
   "source": [
    "## Step 12: Similarity Evaluator\n",
    "\n",
    "How similar is the response to the expected ground truth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59bc50ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### üéØ Similarity Evaluation\n",
       "\n",
       "**Ground Truth:** Olympus Mons on Mars is the largest volcano in the solar system, about 13.6 miles high.\n",
       "\n",
       "**Response:** The largest volcano in the solar system is Olympus Mons on Mars. It is about 13.6 miles (approximately 22 kilometers) high, making it the tallest volcano known in our solar system. \n",
       "\n",
       "Source: fact-010...\n",
       "\n",
       "| Metric | Score |\n",
       "|--------|-------|\n",
       "| **Similarity** | 5.0 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azure.ai.evaluation import SimilarityEvaluator\n",
    "\n",
    "similarity_eval = SimilarityEvaluator(model_config)\n",
    "\n",
    "similarity_result = similarity_eval(\n",
    "    query=sample['query'],\n",
    "    response=sample['response'],\n",
    "    ground_truth=sample['ground_truth']\n",
    ")\n",
    "\n",
    "display(Markdown(f'''\n",
    "### üéØ Similarity Evaluation\n",
    "\n",
    "**Ground Truth:** {sample['ground_truth']}\n",
    "\n",
    "**Response:** {sample['response'][:200]}...\n",
    "\n",
    "| Metric | Score |\n",
    "|--------|-------|\n",
    "| **Similarity** | {similarity_result.get('similarity', 'N/A')} |\n",
    "'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc52d8",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Custom Evaluator\n",
    "\n",
    "Create your own evaluator for domain-specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fda314b",
   "metadata": {},
   "source": [
    "## Step 13: Create a Custom Answer Length Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8f3bdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### üéØ Custom Evaluator: Answer Length\n",
       "\n",
       "| Metric | Value |\n",
       "|--------|-------|\n",
       "| **Length** | 199 characters |\n",
       "| **In Range** | ‚úÖ Yes |\n",
       "\n",
       "**Reason:** Response has 199 characters. ‚úÖ Within range (50-1000).\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class AnswerLengthEvaluator:\n",
    "    \"\"\"Custom evaluator that checks if the answer is within an acceptable length range.\"\"\"\n",
    "    \n",
    "    def __init__(self, min_length: int = 50, max_length: int = 500):\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, *, response: str, **kwargs):\n",
    "        length = len(response)\n",
    "        in_range = self.min_length <= length <= self.max_length\n",
    "        \n",
    "        return {\n",
    "            \"answer_length\": length,\n",
    "            \"length_in_range\": 1 if in_range else 0,\n",
    "            \"length_reason\": f\"Response has {length} characters. {'‚úÖ Within range' if in_range else '‚ùå Outside range'} ({self.min_length}-{self.max_length}).\"\n",
    "        }\n",
    "\n",
    "# Test custom evaluator\n",
    "answer_length_eval = AnswerLengthEvaluator(min_length=50, max_length=1000)\n",
    "length_result = answer_length_eval(response=sample['response'])\n",
    "\n",
    "display(Markdown(f'''\n",
    "### üéØ Custom Evaluator: Answer Length\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Length** | {length_result['answer_length']} characters |\n",
    "| **In Range** | {\"‚úÖ Yes\" if length_result['length_in_range'] else \"‚ùå No\"} |\n",
    "\n",
    "**Reason:** {length_result['length_reason']}\n",
    "'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcfbea2",
   "metadata": {},
   "source": [
    "## Step 14: Create a Citation Check Evaluator\n",
    "\n",
    "Since our agent should cite sources, let's check for citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07edb034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### üéØ Custom Evaluator: Citation Check\n",
       "\n",
       "| Metric | Value |\n",
       "|--------|-------|\n",
       "| **Has Citation** | ‚ùå No |\n",
       "\n",
       "**Reason:** No citation found in response.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "class CitationEvaluator:\n",
    "    \"\"\"Custom evaluator that checks if the response contains citations.\"\"\"\n",
    "    \n",
    "    def __call__(self, *, response: str, **kwargs):\n",
    "        # Look for citation patterns (brackets, source mentions, etc.)\n",
    "        citation_patterns = [\n",
    "            r'\\[\\d+\\]',           # [1], [2], etc.\n",
    "            r'\\[source\\]',         # [source]\n",
    "            r'according to',       # \"according to...\"\n",
    "            r'based on',           # \"based on...\"\n",
    "            r'from the',           # \"from the knowledge base\"\n",
    "            r'knowledge base',     # direct mention\n",
    "            r'space fact',         # mentions source\n",
    "        ]\n",
    "        \n",
    "        has_citation = any(re.search(pattern, response.lower()) for pattern in citation_patterns)\n",
    "        \n",
    "        return {\n",
    "            \"has_citation\": 1 if has_citation else 0,\n",
    "            \"citation_reason\": \"Response contains citation/source reference.\" if has_citation else \"No citation found in response.\"\n",
    "        }\n",
    "\n",
    "# Test citation evaluator\n",
    "citation_eval = CitationEvaluator()\n",
    "citation_result = citation_eval(response=sample['response'])\n",
    "\n",
    "display(Markdown(f'''\n",
    "### üéØ Custom Evaluator: Citation Check\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Has Citation** | {\"‚úÖ Yes\" if citation_result['has_citation'] else \"‚ùå No\"} |\n",
    "\n",
    "**Reason:** {citation_result['citation_reason']}\n",
    "'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebfd9f4",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Batch Evaluation with evaluate()\n",
    "\n",
    "Now let's run all evaluators on the entire test dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d60ec7",
   "metadata": {},
   "source": [
    "## Step 15: Run Batch Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27795e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "print(\"üöÄ Running batch evaluation on test dataset...\")\n",
    "print(\"   This may take a minute...\\n\")\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"test_data.jsonl\",\n",
    "    evaluators={\n",
    "        \"coherence\": coherence_eval,\n",
    "        \"fluency\": fluency_eval,\n",
    "        \"relevance\": relevance_eval,\n",
    "        \"groundedness\": groundedness_eval,\n",
    "        \"similarity\": similarity_eval,\n",
    "        \"answer_length\": answer_length_eval,\n",
    "        \"citation\": citation_eval\n",
    "    },\n",
    "    evaluator_config={\n",
    "        \"groundedness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        },\n",
    "        \"similarity\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    output_path=\"./evaluation_results.json\",\n",
    "    _use_pf_client=False  # Disable promptflow multiprocessing (fixes fork issues in containers/notebooks)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Batch evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0032e04c",
   "metadata": {},
   "source": [
    "## Step 16: View Aggregate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb4c65a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### üìä Aggregate Metrics Summary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Quality Metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_ae44f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_ae44f_level0_col0\" class=\"col_heading level0 col0\" >Metric</th>\n",
       "      <th id=\"T_ae44f_level0_col1\" class=\"col_heading level0 col1\" >Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_ae44f_row0_col0\" class=\"data row0 col0\" >coherence.coherence</td>\n",
       "      <td id=\"T_ae44f_row0_col1\" class=\"data row0 col1\" >4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ae44f_row1_col0\" class=\"data row1 col0\" >coherence.gpt_coherence</td>\n",
       "      <td id=\"T_ae44f_row1_col1\" class=\"data row1 col1\" >4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ae44f_row2_col0\" class=\"data row2 col0\" >fluency.fluency</td>\n",
       "      <td id=\"T_ae44f_row2_col1\" class=\"data row2 col1\" >3.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ae44f_row3_col0\" class=\"data row3 col0\" >fluency.gpt_fluency</td>\n",
       "      <td id=\"T_ae44f_row3_col1\" class=\"data row3 col1\" >3.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ae44f_row4_col0\" class=\"data row4 col0\" >relevance.relevance</td>\n",
       "      <td id=\"T_ae44f_row4_col1\" class=\"data row4 col1\" >4.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_ae44f_row5_col0\" class=\"data row5 col0\" >relevance.gpt_relevance</td>\n",
       "      <td id=\"T_ae44f_row5_col1\" class=\"data row5 col1\" >4.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffff321b2e40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### RAG & Similarity Metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_c2b82\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_c2b82_level0_col0\" class=\"col_heading level0 col0\" >Metric</th>\n",
       "      <th id=\"T_c2b82_level0_col1\" class=\"col_heading level0 col1\" >Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_c2b82_row0_col0\" class=\"data row0 col0\" >groundedness.groundedness</td>\n",
       "      <td id=\"T_c2b82_row0_col1\" class=\"data row0 col1\" >5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c2b82_row1_col0\" class=\"data row1 col0\" >groundedness.gpt_groundedness</td>\n",
       "      <td id=\"T_c2b82_row1_col1\" class=\"data row1 col1\" >5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c2b82_row2_col0\" class=\"data row2 col0\" >similarity.similarity</td>\n",
       "      <td id=\"T_c2b82_row2_col1\" class=\"data row2 col1\" >5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c2b82_row3_col0\" class=\"data row3 col0\" >similarity.gpt_similarity</td>\n",
       "      <td id=\"T_c2b82_row3_col1\" class=\"data row3 col1\" >5.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffff32197ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Custom Metrics"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_dc742\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_dc742_level0_col0\" class=\"col_heading level0 col0\" >Metric</th>\n",
       "      <th id=\"T_dc742_level0_col1\" class=\"col_heading level0 col1\" >Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_dc742_row0_col0\" class=\"data row0 col0\" >answer_length.answer_length</td>\n",
       "      <td id=\"T_dc742_row0_col1\" class=\"data row0 col1\" >196.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_dc742_row1_col0\" class=\"data row1 col0\" >answer_length.length_in_range</td>\n",
       "      <td id=\"T_dc742_row1_col1\" class=\"data row1 col1\" >1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_dc742_row2_col0\" class=\"data row2 col0\" >citation.has_citation</td>\n",
       "      <td id=\"T_dc742_row2_col1\" class=\"data row2 col1\" >0.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffff32197ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from evaluation_helpers import display_metrics_summary\n",
    "\n",
    "# Display formatted metrics\n",
    "display_metrics_summary(result['metrics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f134f3b4",
   "metadata": {},
   "source": [
    "## Step 17: View Row-Level Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac88ad55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### üìã Row-Level Results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/getting-started-with-foundry/16-evaluation/evaluation_helpers.py:99: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  styled_df = df.style.applymap(highlight_scores, subset=score_columns).hide(axis='index')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_96e76_row0_col2, #T_96e76_row0_col4, #T_96e76_row0_col5, #T_96e76_row0_col6, #T_96e76_row1_col2, #T_96e76_row1_col4, #T_96e76_row1_col6, #T_96e76_row2_col2, #T_96e76_row2_col3, #T_96e76_row2_col4, #T_96e76_row2_col5, #T_96e76_row2_col6, #T_96e76_row3_col2, #T_96e76_row3_col4, #T_96e76_row3_col5, #T_96e76_row3_col6, #T_96e76_row4_col2, #T_96e76_row4_col4, #T_96e76_row4_col5, #T_96e76_row4_col6, #T_96e76_row5_col2, #T_96e76_row5_col3, #T_96e76_row5_col4, #T_96e76_row5_col5, #T_96e76_row5_col6 {\n",
       "  background-color: #2e7d32;\n",
       "  color: #e8f5e9;\n",
       "}\n",
       "#T_96e76_row0_col3, #T_96e76_row1_col3, #T_96e76_row3_col3, #T_96e76_row4_col3 {\n",
       "  background-color: #f9a825;\n",
       "  color: #1a1a1a;\n",
       "}\n",
       "#T_96e76_row1_col5 {\n",
       "  background-color: #c62828;\n",
       "  color: #ffebee;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_96e76\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_96e76_level0_col0\" class=\"col_heading level0 col0\" >#</th>\n",
       "      <th id=\"T_96e76_level0_col1\" class=\"col_heading level0 col1\" >Query</th>\n",
       "      <th id=\"T_96e76_level0_col2\" class=\"col_heading level0 col2\" >Coherence</th>\n",
       "      <th id=\"T_96e76_level0_col3\" class=\"col_heading level0 col3\" >Fluency</th>\n",
       "      <th id=\"T_96e76_level0_col4\" class=\"col_heading level0 col4\" >Relevance</th>\n",
       "      <th id=\"T_96e76_level0_col5\" class=\"col_heading level0 col5\" >Groundedness</th>\n",
       "      <th id=\"T_96e76_level0_col6\" class=\"col_heading level0 col6\" >Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_96e76_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_96e76_row0_col1\" class=\"data row0 col1\" >What is the largest volcano in the solar...</td>\n",
       "      <td id=\"T_96e76_row0_col2\" class=\"data row0 col2\" >4.000000</td>\n",
       "      <td id=\"T_96e76_row0_col3\" class=\"data row0 col3\" >3.000000</td>\n",
       "      <td id=\"T_96e76_row0_col4\" class=\"data row0 col4\" >4.000000</td>\n",
       "      <td id=\"T_96e76_row0_col5\" class=\"data row0 col5\" >5.000000</td>\n",
       "      <td id=\"T_96e76_row0_col6\" class=\"data row0 col6\" >5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_96e76_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_96e76_row1_col1\" class=\"data row1 col1\" >How long has Jupiter's Great Red Spot be...</td>\n",
       "      <td id=\"T_96e76_row1_col2\" class=\"data row1 col2\" >4.000000</td>\n",
       "      <td id=\"T_96e76_row1_col3\" class=\"data row1 col3\" >3.000000</td>\n",
       "      <td id=\"T_96e76_row1_col4\" class=\"data row1 col4\" >5.000000</td>\n",
       "      <td id=\"T_96e76_row1_col5\" class=\"data row1 col5\" >nan</td>\n",
       "      <td id=\"T_96e76_row1_col6\" class=\"data row1 col6\" >5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_96e76_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "      <td id=\"T_96e76_row2_col1\" class=\"data row2 col1\" >Why is a day on Venus longer than its ye...</td>\n",
       "      <td id=\"T_96e76_row2_col2\" class=\"data row2 col2\" >4.000000</td>\n",
       "      <td id=\"T_96e76_row2_col3\" class=\"data row2 col3\" >4.000000</td>\n",
       "      <td id=\"T_96e76_row2_col4\" class=\"data row2 col4\" >4.000000</td>\n",
       "      <td id=\"T_96e76_row2_col5\" class=\"data row2 col5\" >5.000000</td>\n",
       "      <td id=\"T_96e76_row2_col6\" class=\"data row2 col6\" >5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_96e76_row3_col0\" class=\"data row3 col0\" >4</td>\n",
       "      <td id=\"T_96e76_row3_col1\" class=\"data row3 col1\" >How much of the solar system's mass does...</td>\n",
       "      <td id=\"T_96e76_row3_col2\" class=\"data row3 col2\" >4.000000</td>\n",
       "      <td id=\"T_96e76_row3_col3\" class=\"data row3 col3\" >3.000000</td>\n",
       "      <td id=\"T_96e76_row3_col4\" class=\"data row3 col4\" >4.000000</td>\n",
       "      <td id=\"T_96e76_row3_col5\" class=\"data row3 col5\" >5.000000</td>\n",
       "      <td id=\"T_96e76_row3_col6\" class=\"data row3 col6\" >5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_96e76_row4_col0\" class=\"data row4 col0\" >5</td>\n",
       "      <td id=\"T_96e76_row4_col1\" class=\"data row4 col1\" >How fast does the International Space St...</td>\n",
       "      <td id=\"T_96e76_row4_col2\" class=\"data row4 col2\" >4.000000</td>\n",
       "      <td id=\"T_96e76_row4_col3\" class=\"data row4 col3\" >3.000000</td>\n",
       "      <td id=\"T_96e76_row4_col4\" class=\"data row4 col4\" >4.000000</td>\n",
       "      <td id=\"T_96e76_row4_col5\" class=\"data row4 col5\" >5.000000</td>\n",
       "      <td id=\"T_96e76_row4_col6\" class=\"data row4 col6\" >5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_96e76_row5_col0\" class=\"data row5 col0\" >6</td>\n",
       "      <td id=\"T_96e76_row5_col1\" class=\"data row5 col1\" >What happens to astronauts' height in sp...</td>\n",
       "      <td id=\"T_96e76_row5_col2\" class=\"data row5 col2\" >4.000000</td>\n",
       "      <td id=\"T_96e76_row5_col3\" class=\"data row5 col3\" >4.000000</td>\n",
       "      <td id=\"T_96e76_row5_col4\" class=\"data row5 col4\" >4.000000</td>\n",
       "      <td id=\"T_96e76_row5_col5\" class=\"data row5 col5\" >5.000000</td>\n",
       "      <td id=\"T_96e76_row5_col6\" class=\"data row5 col6\" >5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0xffff317b42d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from evaluation_helpers import display_row_results\n",
    "\n",
    "# Display row-level results as a table\n",
    "display_row_results(result['rows'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0522834c",
   "metadata": {},
   "source": [
    "## Step 18: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754989f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_helpers import analyze_evaluation_results\n",
    "\n",
    "# Detailed analysis with recommendations\n",
    "analyze_evaluation_results(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f978bc68",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: NLP Evaluators (No Model Required)\n",
    "\n",
    "These evaluators use mathematical formulas rather than AI models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c6922b",
   "metadata": {},
   "source": [
    "## Step 19: F1 Score Evaluator\n",
    "\n",
    "Measures word overlap between response and ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4086c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä F1 Scores (word overlap with ground truth):\n",
      "==================================================\n",
      "Query 1: F1 = 0.636\n",
      "Query 2: F1 = 0.556\n",
      "Query 3: F1 = 0.494\n",
      "Query 4: F1 = 1.000\n",
      "Query 5: F1 = 0.537\n",
      "Query 6: F1 = 0.618\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import F1ScoreEvaluator\n",
    "\n",
    "f1_eval = F1ScoreEvaluator()\n",
    "\n",
    "# Test on all samples\n",
    "print(\"üìä F1 Scores (word overlap with ground truth):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, row in df_test.iterrows():\n",
    "    f1_result = f1_eval(\n",
    "        response=row['response'],\n",
    "        ground_truth=row['ground_truth']\n",
    "    )\n",
    "    print(f\"Query {i+1}: F1 = {f1_result.get('f1_score', 0):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd9ffd9",
   "metadata": {},
   "source": [
    "## Step 20: BLEU Score Evaluator\n",
    "\n",
    "Standard machine translation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f274d16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä BLEU Scores:\n",
      "==================================================\n",
      "Query 1: BLEU = 0.225\n",
      "Query 2: BLEU = 0.306\n",
      "Query 3: BLEU = 0.203\n",
      "Query 4: BLEU = 0.783\n",
      "Query 5: BLEU = 0.142\n",
      "Query 6: BLEU = 0.362\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import BleuScoreEvaluator\n",
    "\n",
    "bleu_eval = BleuScoreEvaluator()\n",
    "\n",
    "print(\"üìä BLEU Scores:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, row in df_test.iterrows():\n",
    "    bleu_result = bleu_eval(\n",
    "        response=row['response'],\n",
    "        ground_truth=row['ground_truth']\n",
    "    )\n",
    "    print(f\"Query {i+1}: BLEU = {bleu_result.get('bleu_score', 0):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e63aa",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Conversation Evaluation\n",
    "\n",
    "Evaluate multi-turn conversations with your agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a264638b",
   "metadata": {},
   "source": [
    "## Step 21: Evaluate a Multi-Turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9701bc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Running multi-turn conversation...\n",
      "Turn 1: What is the largest volcano in the solar system?\n",
      "Turn 2: How high is it?\n",
      "\n",
      "‚úÖ Conversation recorded\n"
     ]
    }
   ],
   "source": [
    "# Simulate a multi-turn conversation\n",
    "print(\"üí¨ Running multi-turn conversation...\")\n",
    "\n",
    "q1 = \"What is the largest volcano in the solar system?\"\n",
    "r1 = ask_space_expert(q1)\n",
    "print(f\"Turn 1: {q1}\")\n",
    "\n",
    "q2 = \"How high is it?\"\n",
    "r2 = ask_space_expert(q2)\n",
    "print(f\"Turn 2: {q2}\")\n",
    "\n",
    "# Format as conversation\n",
    "conversation = {\n",
    "    \"messages\": [\n",
    "        {\"content\": q1, \"role\": \"user\"},\n",
    "        {\"content\": r1, \"role\": \"assistant\", \"context\": \"Mars has the largest volcano in the solar system called Olympus Mons which is about 13.6 miles high.\"},\n",
    "        {\"content\": q2, \"role\": \"user\"},\n",
    "        {\"content\": r2, \"role\": \"assistant\", \"context\": \"Olympus Mons is about 13.6 miles (22 km) high.\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Conversation recorded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e1261e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### üéØ Conversation Groundedness Evaluation\n",
       "\n",
       "**Overall Score:** 3.0\n",
       "\n",
       "**Per-Turn Scores:**\n",
       "| Turn | Score | Result |\n",
       "|------|-------|--------|\n",
       "| Turn 1 | 5.0 | N/A |\n",
       "| Turn 2 | 1.0 | N/A |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the conversation\n",
    "groundedness_conv = groundedness_eval(conversation=conversation)\n",
    "\n",
    "display(Markdown(f'''\n",
    "### üéØ Conversation Groundedness Evaluation\n",
    "\n",
    "**Overall Score:** {groundedness_conv.get('groundedness', 'N/A')}\n",
    "\n",
    "**Per-Turn Scores:**\n",
    "| Turn | Score | Result |\n",
    "|------|-------|--------|\n",
    "| Turn 1 | {groundedness_conv.get('evaluation_per_turn', {}).get('groundedness', [None, None])[0]} | {groundedness_conv.get('evaluation_per_turn', {}).get('groundedness_result', ['N/A', 'N/A'])[0]} |\n",
    "| Turn 2 | {groundedness_conv.get('evaluation_per_turn', {}).get('groundedness', [None, None])[1]} | {groundedness_conv.get('evaluation_per_turn', {}).get('groundedness_result', ['N/A', 'N/A'])[1]} |\n",
    "'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e655d8de",
   "metadata": {},
   "source": [
    "---\n",
    "## üéâ Summary\n",
    "\n",
    "You've learned how to evaluate your AI applications with the Azure AI Evaluation SDK!\n",
    "\n",
    "### Evaluators Used\n",
    "\n",
    "| Category | Evaluators | Purpose |\n",
    "|----------|------------|---------|  \n",
    "| **Quality** | Coherence, Fluency | Response readability |\n",
    "| **Relevance** | Relevance | Does it answer the question? |\n",
    "| **RAG** | Groundedness, Similarity | Factual accuracy, hallucination detection |\n",
    "| **NLP** | F1 Score, BLEU | Mathematical text similarity |\n",
    "| **Custom** | AnswerLength, Citation | Domain-specific requirements |\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Single-row evaluation** = Spot-check individual responses\n",
    "- **Batch evaluation** = Scale to entire test datasets\n",
    "- **Custom evaluators** = Add your own business logic\n",
    "- **Conversation mode** = Evaluate multi-turn interactions\n",
    "- **NLP evaluators** = No model needed for mathematical metrics\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Add **safety evaluators** for content moderation\n",
    "- Log results to **Foundry project** for tracking\n",
    "- Create **CI/CD pipelines** with evaluation gates\n",
    "- Build **custom evaluators** for your domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2770aacd",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7fcd598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove generated files\n",
    "# import os\n",
    "# os.remove('test_data.jsonl')\n",
    "# os.remove('evaluation_results.json')\n",
    "# print(\"üóëÔ∏è Cleanup complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
