{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430d1cf7",
   "metadata": {},
   "source": [
    "# Lab 15: Observability for AI Agents and Workflows\n",
    "\n",
    "> âš ï¸ **Beta**: This notebook is still in development.  \n",
    "\n",
    "This lab demonstrates how to add comprehensive **tracing and observability** to:\n",
    "\n",
    "1. **Simple Agents** - Hello World style single-agent patterns (from Lab 1b)\n",
    "2. **Multi-Agent Workflows** - Complex orchestration with critique loops (from Lab 12)\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| OpenTelemetry Setup | Configure distributed tracing for AI services |\n",
    "| Azure Monitor Integration | Send traces to Application Insights |\n",
    "| Agent Instrumentation | Automatic tracing for Foundry SDK calls |\n",
    "| Custom Spans | Manual span creation for business logic |\n",
    "| Debug Event Emission | Real-time workflow progress tracking |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d7d05",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Complete these labs first:\n",
    "- **Lab 1a** - Landing Zone deployment (APIM Gateway)\n",
    "- **Lab 1b** - Project Spoke deployment (optional, for Foundry agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d9378",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4623cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core tracing dependencies\n",
    "%pip install -q opentelemetry-sdk opentelemetry-api\n",
    "\n",
    "# Azure Monitor exporter (for Application Insights)\n",
    "%pip install -q azure-monitor-opentelemetry\n",
    "\n",
    "# Optional: OTLP exporters for local tracing (Jaeger/Zipkin)\n",
    "%pip install -q opentelemetry-exporter-otlp-proto-grpc opentelemetry-exporter-otlp-proto-http\n",
    "\n",
    "# Agent Framework\n",
    "%pip install -q agent-framework --pre\n",
    "\n",
    "# Azure AI Projects SDK with telemetry support\n",
    "%pip install -q azure-ai-projects==2.0.0b2 azure-identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16677c5d",
   "metadata": {},
   "source": [
    "## Step 2: Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add this directory to path for local imports\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Load .env from parent directory\n",
    "env_path = Path(\"../.env\")\n",
    "if env_path.exists():\n",
    "    for line in env_path.read_text().splitlines():\n",
    "        if '=' in line and not line.startswith('#'):\n",
    "            key, value = line.split('=', 1)\n",
    "            os.environ[key.strip()] = value.strip()\n",
    "\n",
    "# Landing Zone settings (from Lab 1a)\n",
    "APIM_URL = os.environ.get(\"APIM_URL\", \"\")\n",
    "APIM_KEY = os.environ.get(\"APIM_KEY\", \"\")\n",
    "MODEL_NAME = os.environ.get(\"MODEL_NAME\", \"gpt-4.1-mini\")\n",
    "\n",
    "# Spoke settings (from Lab 1b) - optional for Foundry agents\n",
    "SPOKE_ENDPOINT = os.environ.get(\"SPOKE_ENDPOINT\", \"\")\n",
    "SPOKE_PROJECT = os.environ.get(\"SPOKE_PROJECT\", \"\")\n",
    "\n",
    "# Application Insights (optional - for production tracing)\n",
    "APP_INSIGHTS_CONN = os.environ.get(\"APPLICATIONINSIGHTS_CONNECTION_STRING\", \"\")\n",
    "\n",
    "print(\"ðŸ“‹ Configuration:\")\n",
    "print(f\"   APIM URL:         {APIM_URL[:50]}...\" if APIM_URL else \"   âŒ APIM_URL not set\")\n",
    "print(f\"   Model:            {MODEL_NAME}\")\n",
    "print(f\"   App Insights:     {'âœ… Configured' if APP_INSIGHTS_CONN else 'âš ï¸ Not configured (will use console)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b2bd3",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Tracing\n",
    "\n",
    "This is the **critical step** - tracing must be initialized **before** creating any agents or making LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c939d2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Tracing Status: âœ… ACTIVE\n",
      "   Content Recording: âœ… Enabled\n"
     ]
    }
   ],
   "source": [
    "from tracing import setup_tracing, is_tracing_enabled, get_trace_count, reset_trace_count\n",
    "\n",
    "# Enable tracing (will use console if no App Insights configured)\n",
    "os.environ[\"TRACING_ENABLED\"] = \"true\"\n",
    "\n",
    "# Initialize tracing\n",
    "tracing_active = setup_tracing(\n",
    "    service_name=\"foundry-observability-lab\",\n",
    "    enable_content_recording=True  # Record prompts and completions in traces\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ” Tracing Status: {'âœ… ACTIVE' if tracing_active else 'âŒ INACTIVE'}\")\n",
    "print(f\"   Content Recording: {'âœ… Enabled' if tracing_active else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90818c25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Simple Agent with Tracing\n",
    "\n",
    "Let's start with a simple \"Hello World\" agent (similar to Lab 1b) and add observability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d85eb",
   "metadata": {},
   "source": [
    "## 1.1 Create Agent with APIM Gateway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65255a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created agent: HelloWorldAgent\n",
      "   Model: gpt-4.1-mini\n",
      "   Tracing: Enabled\n"
     ]
    }
   ],
   "source": [
    "from agent_framework.openai import OpenAIChatClient\n",
    "from agent_framework import ChatMessage, Role\n",
    "\n",
    "# Create chat client using APIM Gateway\n",
    "# The tracing is automatically instrumented by the Azure SDK settings\n",
    "chat_client = OpenAIChatClient(\n",
    "    model_id=MODEL_NAME,\n",
    "    api_key=\"placeholder\",  # APIM uses header auth\n",
    "    base_url=f\"{APIM_URL}/deployments/{MODEL_NAME}\",\n",
    "    default_headers={\"api-key\": APIM_KEY},\n",
    ")\n",
    "\n",
    "# Create a simple agent\n",
    "hello_agent = chat_client.create_agent(\n",
    "    name=\"HelloWorldAgent\",\n",
    "    instructions=\"\"\"You are a friendly assistant that demonstrates observability.\n",
    "When responding, always mention that you're being traced by OpenTelemetry.\n",
    "Keep responses brief and cheerful.\"\"\",\n",
    "    model=MODEL_NAME,\n",
    ")\n",
    "\n",
    "print(f\"âœ… Created agent: {hello_agent.name}\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Tracing: {'Enabled' if is_tracing_enabled() else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd3149",
   "metadata": {},
   "source": [
    "## 1.2 Invoke Agent (with Automatic Tracing)\n",
    "\n",
    "When we call the agent, OpenTelemetry automatically captures:\n",
    "- Request/response spans\n",
    "- Token usage metrics\n",
    "- Latency information\n",
    "- Prompt and completion content (if enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Reset trace count before our test\n",
    "reset_trace_count()\n",
    "start_traces = get_trace_count()\n",
    "\n",
    "# Call the agent\n",
    "query = \"Hello! Can you tell me about observability in AI systems?\"\n",
    "print(f\"ðŸ§‘ User: {query}\")\n",
    "print(\"\\nâ³ Calling agent (with tracing)...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "response = await hello_agent.run(\n",
    "    [ChatMessage(role=Role.USER, text=query)]\n",
    ")\n",
    "duration_ms = (time.time() - start_time) * 1000\n",
    "\n",
    "# Display response\n",
    "print(f\"ðŸ¤– Agent: {response.text}\")\n",
    "print(f\"\\nðŸ“Š Metrics:\")\n",
    "print(f\"   Duration: {duration_ms:.0f}ms\")\n",
    "print(f\"   Traces recorded: {get_trace_count() - start_traces}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ca576",
   "metadata": {},
   "source": [
    "## 1.3 Add Custom Spans with Decorator\n",
    "\n",
    "For business logic that wraps agent calls, use the `@with_agent_telemetry` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cab4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maf_wrapper import with_agent_telemetry\n",
    "\n",
    "@with_agent_telemetry(\"SummarizationAgent\", \"Summarize user input\")\n",
    "async def summarize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    This function is automatically traced with:\n",
    "    - A parent span named 'SummarizationAgent.summarize_text'\n",
    "    - Input text captured in span attributes\n",
    "    - Output captured in span attributes\n",
    "    - Timing and error tracking\n",
    "    \"\"\"\n",
    "    response = await hello_agent.run([\n",
    "        ChatMessage(\n",
    "            role=Role.USER,\n",
    "            text=f\"Summarize this in one sentence: {text}\"\n",
    "        )\n",
    "    ])\n",
    "    return response.text\n",
    "\n",
    "# Test the decorated function\n",
    "long_text = \"\"\"\n",
    "OpenTelemetry is an observability framework for cloud-native software.\n",
    "It provides APIs, libraries, and tools to collect distributed traces,\n",
    "metrics, and logs from your applications. The project is a merger of\n",
    "OpenTracing and OpenCensus projects.\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“ Input text (truncated):\")\n",
    "print(f\"   {long_text.strip()[:100]}...\")\n",
    "print(\"\\nâ³ Calling decorated function...\\n\")\n",
    "\n",
    "summary = await summarize_text(long_text)\n",
    "print(f\"ðŸ“‹ Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea63914",
   "metadata": {},
   "source": [
    "## 1.4 Manual Span Creation\n",
    "\n",
    "For fine-grained control, create spans manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc9aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tracing import get_tracer\n",
    "from opentelemetry import trace as otel_trace\n",
    "\n",
    "tracer = get_tracer(\"my_custom_logic\")\n",
    "\n",
    "async def process_with_custom_spans(user_input: str) -> dict:\n",
    "    \"\"\"\n",
    "    Demonstrates manual span creation for complex business logic.\n",
    "    \"\"\"\n",
    "    # Parent span for the entire operation\n",
    "    with tracer.start_as_current_span(\n",
    "        \"process_user_request\",\n",
    "        kind=otel_trace.SpanKind.INTERNAL\n",
    "    ) as parent_span:\n",
    "        parent_span.set_attribute(\"user.input_length\", len(user_input))\n",
    "        \n",
    "        # Child span for preprocessing\n",
    "        with tracer.start_as_current_span(\"preprocess\") as prep_span:\n",
    "            cleaned_input = user_input.strip().lower()\n",
    "            prep_span.set_attribute(\"preprocessing.result_length\", len(cleaned_input))\n",
    "        \n",
    "        # Child span for agent call\n",
    "        with tracer.start_as_current_span(\n",
    "            \"call_agent\",\n",
    "            kind=otel_trace.SpanKind.CLIENT\n",
    "        ) as agent_span:\n",
    "            agent_span.set_attribute(\"agent.name\", \"HelloWorldAgent\")\n",
    "            agent_span.set_attribute(\"gen_ai.prompt\", cleaned_input[:200])\n",
    "            \n",
    "            response = await hello_agent.run([\n",
    "                ChatMessage(role=Role.USER, text=cleaned_input)\n",
    "            ])\n",
    "            \n",
    "            agent_span.set_attribute(\"gen_ai.completion\", response.text[:200])\n",
    "        \n",
    "        # Child span for postprocessing\n",
    "        with tracer.start_as_current_span(\"postprocess\") as post_span:\n",
    "            result = {\n",
    "                \"input\": user_input,\n",
    "                \"response\": response.text,\n",
    "                \"word_count\": len(response.text.split())\n",
    "            }\n",
    "            post_span.set_attribute(\"postprocessing.word_count\", result[\"word_count\"])\n",
    "        \n",
    "        parent_span.set_status(otel_trace.StatusCode.OK)\n",
    "        return result\n",
    "\n",
    "# Test manual spans\n",
    "result = await process_with_custom_spans(\"What is distributed tracing?\")\n",
    "print(f\"âœ… Processed request with {result['word_count']} words in response\")\n",
    "print(f\"ðŸ“Š Total traces recorded: {get_trace_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43880d94",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Multi-Agent Workflow with Tracing\n",
    "\n",
    "Now let's add observability to a more complex multi-agent workflow (similar to Lab 12)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c2c234",
   "metadata": {},
   "source": [
    "## 2.1 Create Debug Event Emitter\n",
    "\n",
    "For real-time workflow monitoring, we use a debug event emitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6991f778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Debug event emitter created\n"
     ]
    }
   ],
   "source": [
    "from debug_events import DebugEventEmitter, create_debug_emitter\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "\n",
    "# Create an emitter that displays events in real-time\n",
    "def display_event(event: dict):\n",
    "    \"\"\"Display workflow events in the notebook.\"\"\"\n",
    "    event_type = event.get(\"type\", \"unknown\")\n",
    "    \n",
    "    # Color-code by event type\n",
    "    colors = {\n",
    "        \"workflow_started\": \"#4CAF50\",\n",
    "        \"workflow_completed\": \"#4CAF50\",\n",
    "        \"phase_started\": \"#2196F3\",\n",
    "        \"llm_call_started\": \"#FF9800\",\n",
    "        \"llm_call_completed\": \"#8BC34A\",\n",
    "        \"llm_call_failed\": \"#f44336\",\n",
    "        \"search_started\": \"#9C27B0\",\n",
    "        \"search_completed\": \"#9C27B0\",\n",
    "    }\n",
    "    color = colors.get(event_type, \"#666\")\n",
    "    \n",
    "    # Format event for display\n",
    "    summary = \"\"\n",
    "    if \"agent\" in event:\n",
    "        summary = f\"Agent: {event['agent']}\"\n",
    "    if \"duration_ms\" in event:\n",
    "        summary += f\" ({event['duration_ms']}ms)\"\n",
    "    if \"description\" in event:\n",
    "        summary = event[\"description\"]\n",
    "    \n",
    "    print(f\"  ðŸ“ [{event_type}] {summary}\")\n",
    "\n",
    "# Create emitter with our display callback\n",
    "emitter = create_debug_emitter(callback=display_event)\n",
    "print(\"âœ… Debug event emitter created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b71add",
   "metadata": {},
   "source": [
    "## 2.2 Workflow Tracer Context Manager\n",
    "\n",
    "The `WorkflowTracer` provides automatic span management for multi-step workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "049f2249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Workflow function defined\n"
     ]
    }
   ],
   "source": [
    "from maf_wrapper import WorkflowTracer\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Define some models for structured output\n",
    "class SlideOutline(BaseModel):\n",
    "    position: int\n",
    "    topic: str\n",
    "    search_keywords: List[str] = Field(default_factory=list)\n",
    "\n",
    "class PresentationOutline(BaseModel):\n",
    "    title: str\n",
    "    narrative: str\n",
    "    slides: List[SlideOutline]\n",
    "\n",
    "# Create a traced workflow\n",
    "async def build_presentation_workflow(topic: str) -> dict:\n",
    "    \"\"\"\n",
    "    Demonstrates a multi-step workflow with comprehensive tracing.\n",
    "    \n",
    "    This simulates the pattern from Lab 12 where multiple agents\n",
    "    collaborate to build a presentation.\n",
    "    \"\"\"\n",
    "    results = {\"topic\": topic, \"slides\": []}\n",
    "    \n",
    "    async with WorkflowTracer(\n",
    "        \"presentation_builder\",\n",
    "        total_steps=3,\n",
    "        event_callback=display_event\n",
    "    ) as wt:\n",
    "        \n",
    "        # Step 1: Planning\n",
    "        wt.start_step(\"planning\", \"Generate presentation outline\")\n",
    "        \n",
    "        emitter.llm_call_started(\n",
    "            agent=\"PlannerAgent\",\n",
    "            task=\"Generate outline\",\n",
    "            prompt_preview=f\"Create outline for: {topic}\",\n",
    "            response_format=\"PresentationOutline\"\n",
    "        )\n",
    "        \n",
    "        # Call the planner (using our simple agent for demo)\n",
    "        import time\n",
    "        start = time.time()\n",
    "        \n",
    "        plan_response = await hello_agent.run([\n",
    "            ChatMessage(\n",
    "                role=Role.USER,\n",
    "                text=f\"Create a 3-slide outline for a presentation about: {topic}. List each slide topic briefly.\"\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        duration = int((time.time() - start) * 1000)\n",
    "        emitter.llm_call_completed(\n",
    "            agent=\"PlannerAgent\",\n",
    "            duration_ms=duration,\n",
    "            response_preview=plan_response.text[:200]\n",
    "        )\n",
    "        \n",
    "        results[\"outline\"] = plan_response.text\n",
    "        wt.complete_step(success=True, details={\"outline_length\": len(plan_response.text)})\n",
    "        \n",
    "        # Step 2: Search (simulated)\n",
    "        wt.start_step(\"search\", \"Find relevant images\")\n",
    "        \n",
    "        emitter.search_started(query=f\"{topic} images\")\n",
    "        await asyncio.sleep(0.5)  # Simulate search\n",
    "        emitter.search_completed(\n",
    "            query=f\"{topic} images\",\n",
    "            result_count=5,\n",
    "            duration_ms=500\n",
    "        )\n",
    "        \n",
    "        wt.complete_step(success=True, details={\"images_found\": 5})\n",
    "        \n",
    "        # Step 3: Assembly\n",
    "        wt.start_step(\"assembly\", \"Assemble final presentation\")\n",
    "        \n",
    "        # Simulate selection with review\n",
    "        for i in range(3):\n",
    "            emitter.selection_made(\n",
    "                position=i + 1,\n",
    "                selected_id=f\"image_{i}\",\n",
    "                reason=f\"Best match for slide {i + 1}\"\n",
    "            )\n",
    "            emitter.review_completed(\n",
    "                position=i + 1,\n",
    "                approved=True,\n",
    "                feedback=\"Image matches the topic well\"\n",
    "            )\n",
    "            results[\"slides\"].append({\"position\": i + 1, \"image_id\": f\"image_{i}\"})\n",
    "        \n",
    "        wt.complete_step(success=True, details={\"slides_assembled\": 3})\n",
    "    \n",
    "    return results\n",
    "\n",
    "import asyncio\n",
    "print(\"âœ… Workflow function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ab590",
   "metadata": {},
   "source": [
    "## 2.3 Run the Traced Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cebdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset trace count\n",
    "reset_trace_count()\n",
    "\n",
    "print(\"ðŸš€ Starting traced workflow...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "result = await build_presentation_workflow(\"OpenTelemetry and Distributed Tracing\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nâœ… Workflow completed!\")\n",
    "print(f\"   Slides created: {len(result['slides'])}\")\n",
    "print(f\"   Traces recorded: {get_trace_count()}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Outline preview:\")\n",
    "print(f\"   {result['outline'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a82f9",
   "metadata": {},
   "source": [
    "## 2.4 View All Captured Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "993b7b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š All captured events (10 total):\n",
      "============================================================\n",
      "  [llm_call_started    ] {\"agent\": \"PlannerAgent\", \"task\": \"Generate outline\", \"prompt_preview\": \"Create outline for: OpenTel\n",
      "  [llm_call_completed  ] {\"agent\": \"PlannerAgent\", \"duration_ms\": 1580, \"response_preview\": \"Sure! Here's a cheerful 3-slide \n",
      "  [search_started      ] {\"query\": \"OpenTelemetry and Distributed Tracing images\"}\n",
      "  [search_completed    ] {\"query\": \"OpenTelemetry and Distributed Tracing images\", \"result_count\": 5, \"duration_ms\": 500, \"re\n",
      "  [selection_made      ] {\"position\": 1, \"selected_id\": \"image_0\", \"reason\": \"Best match for slide 1\"}\n",
      "  [review_completed    ] {\"position\": 1, \"approved\": true, \"feedback\": \"Image matches the topic well\"}\n",
      "  [selection_made      ] {\"position\": 2, \"selected_id\": \"image_1\", \"reason\": \"Best match for slide 2\"}\n",
      "  [review_completed    ] {\"position\": 2, \"approved\": true, \"feedback\": \"Image matches the topic well\"}\n",
      "  [selection_made      ] {\"position\": 3, \"selected_id\": \"image_2\", \"reason\": \"Best match for slide 3\"}\n",
      "  [review_completed    ] {\"position\": 3, \"approved\": true, \"feedback\": \"Image matches the topic well\"}\n"
     ]
    }
   ],
   "source": [
    "print(f\"ðŸ“Š All captured events ({len(emitter.events)} total):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for event in emitter.events:\n",
    "    event_type = event.get(\"type\", \"unknown\")\n",
    "    timestamp = event.get(\"timestamp\", 0)\n",
    "    \n",
    "    # Format event details\n",
    "    details = {k: v for k, v in event.items() if k not in [\"type\", \"timestamp\"]}\n",
    "    details_str = json.dumps(details, default=str)[:100]\n",
    "    \n",
    "    print(f\"  [{event_type:20}] {details_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20509b3a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## What We Learned\n",
    "\n",
    "| Concept | Implementation |\n",
    "|---------|----------------|\n",
    "| **Tracing Setup** | `setup_tracing()` - Call once at startup |\n",
    "| **Auto-Instrumentation** | Azure SDK automatically traces all LLM calls |\n",
    "| **Custom Spans** | `get_tracer()` + context managers |\n",
    "| **Decorated Functions** | `@with_agent_telemetry()` decorator |\n",
    "| **Workflow Tracing** | `WorkflowTracer` context manager |\n",
    "| **Debug Events** | `DebugEventEmitter` for real-time monitoring |\n",
    "\n",
    "## Key Files in This Lab\n",
    "\n",
    "| File | Purpose |\n",
    "|------|--------|\n",
    "| `tracing.py` | OpenTelemetry setup and configuration |\n",
    "| `maf_wrapper.py` | Agent telemetry decorators and workflow tracer |\n",
    "| `debug_events.py` | Event emission for real-time monitoring |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Deploy Application Insights** for production monitoring\n",
    "2. **Create dashboards** for agent performance metrics\n",
    "3. **Set up alerts** for error rates and latency spikes\n",
    "4. **Add custom metrics** for business KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d78a7dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Final Statistics:\n",
      "   Total traces recorded: 1\n",
      "   Debug events captured: 10\n",
      "   Tracing active: True\n"
     ]
    }
   ],
   "source": [
    "# Final trace count\n",
    "print(f\"\\nðŸ“Š Final Statistics:\")\n",
    "print(f\"   Total traces recorded: {get_trace_count()}\")\n",
    "print(f\"   Debug events captured: {len(emitter.events)}\")\n",
    "print(f\"   Tracing active: {is_tracing_enabled()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
